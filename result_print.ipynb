{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Eval command generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['save_ao/0/checkpoint-2925',\n",
       " 'save_ao/1/checkpoint-2475',\n",
       " 'save_ao/2/checkpoint-2250',\n",
       " 'save_ao/3/checkpoint-1800',\n",
       " 'save_ao/4/checkpoint-1575',\n",
       " 'save_mm/0/checkpoint-4480',\n",
       " 'save_mm/1/checkpoint-4480',\n",
       " 'save_mm/2/checkpoint-2912',\n",
       " 'save_mm/3/checkpoint-4256',\n",
       " 'save_mm/4/checkpoint-3584']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "sorted(glob.glob('save_*/[0-4]/checkpoint-*'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    CUDA_VISIBLE_DEVICES=0 python eval.py --output_dir=./save_ao/0/eval      --model_name_or_path=save_ao/0/checkpoint-2925     --train_manifest_path=dataset/mm_train_metadata.csv     --valid_manifest_path=dataset/mm_valid_metadata.csv     --test_manifest_path=dataset/mm_test_metadata.csv     --num_workers=8 --preprocessing_num_workers=8      --audio_column_name=audio_path --text_column_name=text_path  --video_column_name=lip_image_path     --per_device_train_batch_size=16 --per_device_eval_batch_size=16     --dataloader_num_workers=32 --dataloader_pin_memory     --seed=0 --num_train_epochs=20 --learning_rate=5e-5     --fp16 --fp16_backend=amp     --logging_strategy=steps --logging_steps=10 --report_to=tensorboard     --evaluation_strategy=epoch --eval_steps=1 --eval_accumulation_steps=100     --save_steps=1 --save_strategy=epoch --save_total_limit=1     --metric_for_best_model=mer --greater_is_better=False --load_best_model_at_end=True\n",
      "\n",
      "\n",
      "    CUDA_VISIBLE_DEVICES=0 python eval.py --output_dir=./save_ao/1/eval      --model_name_or_path=save_ao/1/checkpoint-2475     --train_manifest_path=dataset/mm_train_metadata.csv     --valid_manifest_path=dataset/mm_valid_metadata.csv     --test_manifest_path=dataset/mm_test_metadata.csv     --num_workers=8 --preprocessing_num_workers=8      --audio_column_name=audio_path --text_column_name=text_path  --video_column_name=lip_image_path     --per_device_train_batch_size=16 --per_device_eval_batch_size=16     --dataloader_num_workers=32 --dataloader_pin_memory     --seed=0 --num_train_epochs=20 --learning_rate=5e-5     --fp16 --fp16_backend=amp     --logging_strategy=steps --logging_steps=10 --report_to=tensorboard     --evaluation_strategy=epoch --eval_steps=1 --eval_accumulation_steps=100     --save_steps=1 --save_strategy=epoch --save_total_limit=1     --metric_for_best_model=mer --greater_is_better=False --load_best_model_at_end=True\n",
      "\n",
      "\n",
      "    CUDA_VISIBLE_DEVICES=0 python eval.py --output_dir=./save_ao/2/eval      --model_name_or_path=save_ao/2/checkpoint-2250     --train_manifest_path=dataset/mm_train_metadata.csv     --valid_manifest_path=dataset/mm_valid_metadata.csv     --test_manifest_path=dataset/mm_test_metadata.csv     --num_workers=8 --preprocessing_num_workers=8      --audio_column_name=audio_path --text_column_name=text_path  --video_column_name=lip_image_path     --per_device_train_batch_size=16 --per_device_eval_batch_size=16     --dataloader_num_workers=32 --dataloader_pin_memory     --seed=0 --num_train_epochs=20 --learning_rate=5e-5     --fp16 --fp16_backend=amp     --logging_strategy=steps --logging_steps=10 --report_to=tensorboard     --evaluation_strategy=epoch --eval_steps=1 --eval_accumulation_steps=100     --save_steps=1 --save_strategy=epoch --save_total_limit=1     --metric_for_best_model=mer --greater_is_better=False --load_best_model_at_end=True\n",
      "\n",
      "\n",
      "    CUDA_VISIBLE_DEVICES=0 python eval.py --output_dir=./save_ao/3/eval      --model_name_or_path=save_ao/3/checkpoint-1800     --train_manifest_path=dataset/mm_train_metadata.csv     --valid_manifest_path=dataset/mm_valid_metadata.csv     --test_manifest_path=dataset/mm_test_metadata.csv     --num_workers=8 --preprocessing_num_workers=8      --audio_column_name=audio_path --text_column_name=text_path  --video_column_name=lip_image_path     --per_device_train_batch_size=16 --per_device_eval_batch_size=16     --dataloader_num_workers=32 --dataloader_pin_memory     --seed=0 --num_train_epochs=20 --learning_rate=5e-5     --fp16 --fp16_backend=amp     --logging_strategy=steps --logging_steps=10 --report_to=tensorboard     --evaluation_strategy=epoch --eval_steps=1 --eval_accumulation_steps=100     --save_steps=1 --save_strategy=epoch --save_total_limit=1     --metric_for_best_model=mer --greater_is_better=False --load_best_model_at_end=True\n",
      "\n",
      "\n",
      "    CUDA_VISIBLE_DEVICES=0 python eval.py --output_dir=./save_ao/4/eval      --model_name_or_path=save_ao/4/checkpoint-1575     --train_manifest_path=dataset/mm_train_metadata.csv     --valid_manifest_path=dataset/mm_valid_metadata.csv     --test_manifest_path=dataset/mm_test_metadata.csv     --num_workers=8 --preprocessing_num_workers=8      --audio_column_name=audio_path --text_column_name=text_path  --video_column_name=lip_image_path     --per_device_train_batch_size=16 --per_device_eval_batch_size=16     --dataloader_num_workers=32 --dataloader_pin_memory     --seed=0 --num_train_epochs=20 --learning_rate=5e-5     --fp16 --fp16_backend=amp     --logging_strategy=steps --logging_steps=10 --report_to=tensorboard     --evaluation_strategy=epoch --eval_steps=1 --eval_accumulation_steps=100     --save_steps=1 --save_strategy=epoch --save_total_limit=1     --metric_for_best_model=mer --greater_is_better=False --load_best_model_at_end=True\n",
      "\n",
      "\n",
      "    CUDA_VISIBLE_DEVICES=0 python eval.py --output_dir=./save_mm/0/eval      --model_name_or_path=save_mm/0/checkpoint-4480     --train_manifest_path=dataset/mm_train_metadata.csv     --valid_manifest_path=dataset/mm_valid_metadata.csv     --test_manifest_path=dataset/mm_test_metadata.csv     --num_workers=8 --preprocessing_num_workers=8 --use_video     --audio_column_name=audio_path --text_column_name=text_path  --video_column_name=lip_image_path     --per_device_train_batch_size=16 --per_device_eval_batch_size=16     --dataloader_num_workers=32 --dataloader_pin_memory     --seed=0 --num_train_epochs=20 --learning_rate=5e-5     --fp16 --fp16_backend=amp     --logging_strategy=steps --logging_steps=10 --report_to=tensorboard     --evaluation_strategy=epoch --eval_steps=1 --eval_accumulation_steps=100     --save_steps=1 --save_strategy=epoch --save_total_limit=1     --metric_for_best_model=mer --greater_is_better=False --load_best_model_at_end=True\n",
      "\n",
      "\n",
      "    CUDA_VISIBLE_DEVICES=0 python eval.py --output_dir=./save_mm/1/eval      --model_name_or_path=save_mm/1/checkpoint-4480     --train_manifest_path=dataset/mm_train_metadata.csv     --valid_manifest_path=dataset/mm_valid_metadata.csv     --test_manifest_path=dataset/mm_test_metadata.csv     --num_workers=8 --preprocessing_num_workers=8 --use_video     --audio_column_name=audio_path --text_column_name=text_path  --video_column_name=lip_image_path     --per_device_train_batch_size=16 --per_device_eval_batch_size=16     --dataloader_num_workers=32 --dataloader_pin_memory     --seed=0 --num_train_epochs=20 --learning_rate=5e-5     --fp16 --fp16_backend=amp     --logging_strategy=steps --logging_steps=10 --report_to=tensorboard     --evaluation_strategy=epoch --eval_steps=1 --eval_accumulation_steps=100     --save_steps=1 --save_strategy=epoch --save_total_limit=1     --metric_for_best_model=mer --greater_is_better=False --load_best_model_at_end=True\n",
      "\n",
      "\n",
      "    CUDA_VISIBLE_DEVICES=0 python eval.py --output_dir=./save_mm/2/eval      --model_name_or_path=save_mm/2/checkpoint-2912     --train_manifest_path=dataset/mm_train_metadata.csv     --valid_manifest_path=dataset/mm_valid_metadata.csv     --test_manifest_path=dataset/mm_test_metadata.csv     --num_workers=8 --preprocessing_num_workers=8 --use_video     --audio_column_name=audio_path --text_column_name=text_path  --video_column_name=lip_image_path     --per_device_train_batch_size=16 --per_device_eval_batch_size=16     --dataloader_num_workers=32 --dataloader_pin_memory     --seed=0 --num_train_epochs=20 --learning_rate=5e-5     --fp16 --fp16_backend=amp     --logging_strategy=steps --logging_steps=10 --report_to=tensorboard     --evaluation_strategy=epoch --eval_steps=1 --eval_accumulation_steps=100     --save_steps=1 --save_strategy=epoch --save_total_limit=1     --metric_for_best_model=mer --greater_is_better=False --load_best_model_at_end=True\n",
      "\n",
      "\n",
      "    CUDA_VISIBLE_DEVICES=0 python eval.py --output_dir=./save_mm/3/eval      --model_name_or_path=save_mm/3/checkpoint-4256     --train_manifest_path=dataset/mm_train_metadata.csv     --valid_manifest_path=dataset/mm_valid_metadata.csv     --test_manifest_path=dataset/mm_test_metadata.csv     --num_workers=8 --preprocessing_num_workers=8 --use_video     --audio_column_name=audio_path --text_column_name=text_path  --video_column_name=lip_image_path     --per_device_train_batch_size=16 --per_device_eval_batch_size=16     --dataloader_num_workers=32 --dataloader_pin_memory     --seed=0 --num_train_epochs=20 --learning_rate=5e-5     --fp16 --fp16_backend=amp     --logging_strategy=steps --logging_steps=10 --report_to=tensorboard     --evaluation_strategy=epoch --eval_steps=1 --eval_accumulation_steps=100     --save_steps=1 --save_strategy=epoch --save_total_limit=1     --metric_for_best_model=mer --greater_is_better=False --load_best_model_at_end=True\n",
      "\n",
      "\n",
      "    CUDA_VISIBLE_DEVICES=0 python eval.py --output_dir=./save_mm/4/eval      --model_name_or_path=save_mm/4/checkpoint-3584     --train_manifest_path=dataset/mm_train_metadata.csv     --valid_manifest_path=dataset/mm_valid_metadata.csv     --test_manifest_path=dataset/mm_test_metadata.csv     --num_workers=8 --preprocessing_num_workers=8 --use_video     --audio_column_name=audio_path --text_column_name=text_path  --video_column_name=lip_image_path     --per_device_train_batch_size=16 --per_device_eval_batch_size=16     --dataloader_num_workers=32 --dataloader_pin_memory     --seed=0 --num_train_epochs=20 --learning_rate=5e-5     --fp16 --fp16_backend=amp     --logging_strategy=steps --logging_steps=10 --report_to=tensorboard     --evaluation_strategy=epoch --eval_steps=1 --eval_accumulation_steps=100     --save_steps=1 --save_strategy=epoch --save_total_limit=1     --metric_for_best_model=mer --greater_is_better=False --load_best_model_at_end=True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "template = \"\"\"\n",
    "    CUDA_VISIBLE_DEVICES=0 python eval.py --output_dir=./{}/{}/eval  \\\n",
    "    --model_name_or_path={} \\\n",
    "    --train_manifest_path=dataset/mm_train_metadata.csv \\\n",
    "    --valid_manifest_path=dataset/mm_valid_metadata.csv \\\n",
    "    --test_manifest_path=dataset/mm_test_metadata.csv \\\n",
    "    --num_workers=8 --preprocessing_num_workers=8 {} \\\n",
    "    --audio_column_name=audio_path --text_column_name=text_path  --video_column_name=lip_image_path \\\n",
    "    --per_device_train_batch_size=16 --per_device_eval_batch_size=16 \\\n",
    "    --dataloader_num_workers=32 --dataloader_pin_memory \\\n",
    "    --seed=0 --num_train_epochs=20 --learning_rate=5e-5 \\\n",
    "    --fp16 --fp16_backend=amp \\\n",
    "    --logging_strategy=steps --logging_steps=10 --report_to=tensorboard \\\n",
    "    --evaluation_strategy=epoch --eval_steps=1 --eval_accumulation_steps=100 \\\n",
    "    --save_steps=1 --save_strategy=epoch --save_total_limit=1 \\\n",
    "    --metric_for_best_model=mer --greater_is_better=False --load_best_model_at_end=True\n",
    "\"\"\"\n",
    "\n",
    "for path in sorted(glob.glob('save_*/[0-4]/checkpoint-*')):\n",
    "    top_folder, seed_number = path.split('/')[:2]\n",
    "    print(template.format(top_folder, seed_number, path, '--use_video' if 'save_mm' in path else ''))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Eval result print\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save_ao/0/eval/all_results.json 0.03737415528892567 0.03794113544427396\n",
      "save_ao/1/eval/all_results.json 0.03889118742242449 0.03905705119263496\n",
      "save_ao/2/eval/all_results.json 0.04109777961660461 0.04142837215790208\n",
      "save_ao/3/eval/all_results.json 0.03902909943456075 0.039336030129725204\n",
      "save_ao/4/eval/all_results.json 0.04661426010205489 0.047286929836797324\n",
      "save_mm/0/eval/all_results.json 0.03392635498551924 0.03431440926210071\n",
      "save_mm/1/eval/all_results.json 0.03516756309474555 0.03556981447900683\n",
      "save_mm/2/eval/all_results.json 0.036270859191835605 0.03626726182173246\n",
      "save_mm/3/eval/all_results.json 0.033236794924837956 0.033477472450829965\n",
      "save_mm/4/eval/all_results.json 0.03516756309474555 0.035011856604826336\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "for path in sorted(glob.glob('save_*/[0-4]/eval/all_results.json')):\n",
    "    result = json.load(open(path))\n",
    "    print(path, result['eval_cer'], result['eval_mer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
